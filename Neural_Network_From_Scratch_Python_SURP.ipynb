{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFHoITSQaBbk/FzVGWKWOM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnthonyCampos123/SURP-Neural-Network-Model-Distillation-and-Pruning-for-Pattern-Recognition-and-Recommendations/blob/main/Neural_Network_From_Scratch_Python_SURP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IystFhlfCrMJ",
        "outputId": "5dac18ae-43cd-4548-9537-5e649bd4ca8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n",
            "[4.8   1.21  2.385]\n",
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n",
            "[[ 0.50310004 -1.04185009 -2.03875005]\n",
            " [ 0.24339998 -2.73320007 -5.76329994]\n",
            " [-0.99314     1.41254002 -0.35654999]]\n",
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.3333332  0.3333332  0.33333364]\n",
            " [0.3333329  0.33333293 0.3333342 ]\n",
            " [0.3333326  0.33333263 0.33333477]\n",
            " [0.33333233 0.3333324  0.33333528]]\n",
            "Loss: 1.0986104\n",
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n",
            "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
            " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
            " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n",
            "[[8.395]\n",
            " [7.29 ]\n",
            " [2.487]]\n",
            "start here\n",
            "acc: 1.0\n",
            "[0.7 0.5 0.9]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "#!pip install nnfs\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# neuron in a densely connected feed-forward multilayer perceptron model, neuron will take in 3 inputs\n",
        "# each unique input is going to have a unique weight associated to it\n",
        "# inputs and weights are arbitrary in this example\n",
        "# each unique neuron has a unique bias\n",
        "\n",
        "# fully connected neural network\n",
        "# selected 3 neurons with 4 inputs each (each neuron will have a unique set of weights for each unique input)\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "layer_outputs = [] # output of current layer\n",
        "for neuron_weights, neuron_bias in zip(weights, biases): # zip comhines 2 lists (lists of lists)\n",
        "  neuron_output = 0 # output of current neuron\n",
        "  for n_input, weight in zip(inputs, neuron_weights):\n",
        "      neuron_output += n_input*weight\n",
        "  neuron_output += neuron_bias\n",
        "  layer_outputs.append(neuron_output)\n",
        "\n",
        "print(layer_outputs)\n",
        "\n",
        "# shape of arrays:\n",
        "# given an array, l = [1, 5, 6, 2],\n",
        "# shape: (4,)\n",
        "# type: 1d array, vector (in math)\n",
        "# given an array, list_of_lists = [[1,5,6,7],[3,2,1,3]],\n",
        "# shape: (2,4)\n",
        "# type: 2d array, matrix (in math)\n",
        "# thus, arrays must be homologous (at each dimension, must have the same size)\n",
        "# given an array list_of_lists_lists = [\n",
        "#                                       [[1,5,6,2],[3,2,1,3]],\n",
        "#                                       [[5,2,1,2],[6,4,8,4]],\n",
        "#                                       [[2,8,5,3],[1,1,9,4]]\n",
        "#                                                            ]\n",
        "# shape: (3,2,4)\n",
        "# type: 3d array\n",
        "\n",
        "# tensors: objects that can be represented as an array (within the context of deep learning)\n",
        "# dot product, a=[1,2,3], b=[2,3,4], a . b = 1*2 + 2*3 + 3*4\n",
        "\n",
        "# second example:\n",
        "# weights are a matrix containing vectors, 3 weights, 3 neurons\n",
        "inputs1 = [1, 2, 3, 2.5]\n",
        "weights1 = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
        "biases1 = [2,3,0.5]\n",
        "\n",
        "# dot product is done three times\n",
        "\n",
        "#output1 = np.dot(weights1, inputs1) + biases1 ######*****************\n",
        "print(output1)\n",
        "\n",
        "# third example: (batches)\n",
        "# features #are inputs\n",
        "# larger batch size, less movement in fitment line\n",
        "# not all samples passsed at once, bad for generaliation of out-of-sample data\n",
        "\n",
        "inputs2 = [[1, 2, 3, 2.5],\n",
        "           [2.0, 5.0, -1.0,2.0],\n",
        "           [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "weights2 = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
        "biases2 = [2,3,0.5]\n",
        "\n",
        "# matrix product, take transpose of weights (to avoid shape error)\n",
        "\n",
        "#output2 = np.dot(inputs2, np.array(weights).T) + biases2 ######*****************\n",
        "print(output2)\n",
        "\n",
        "\n",
        "# fourth example:\n",
        "\n",
        "biases3 = [-1,2,-0.5]\n",
        "\n",
        "inputs3 = [[1, 2, 3, 2.5],\n",
        "           [2.0, 5.0, -1.0,2.0],\n",
        "           [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "weights3 = [[0.1, -0.14, 0.5], [-0.5, 0.12, -0.33], [-0.44, 0.73, -0.13]]\n",
        "\n",
        "\n",
        "#layer1_outputs = np.dot(inputs2, np.array(weights).T) + biases2 ######*****************\n",
        "\n",
        "layer2_outputs = np.dot(layer1_outputs, np.array(weights3).T) + biases3\n",
        "\n",
        "print(layer2_outputs)\n",
        "\n",
        "# fifth example (object oriented programming)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# standard in machine learning to denote inputs as X\n",
        "nnfs.init()\n",
        "\n",
        "# X = [[1, 2, 3, 2.5],\n",
        "#            [2.0, 5.0, -1.0,2.0],\n",
        "#            [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "# X, y = spiral_data(100,3)\n",
        "\n",
        "\n",
        "# spiral dataset\n",
        "# def create_data(points, classes):\n",
        "#   X = np.zeros((points*classes, 2))\n",
        "#   y = np.zeros(points*classes, dtype='uint8')\n",
        "#   for class_number in range(classes):\n",
        "#       ix = range(points*class_number, points*(class_number+1))\n",
        "#       r = np.linspace(0.0,1,points) # radius\n",
        "#       t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
        "#       X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
        "#       y[ix] = class_number\n",
        "#   return X, y\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# print(\"here\")\n",
        "# X, y = create_data(100,3)\n",
        "# plt.scatter(X[:,0], X[:,1])\n",
        "# plt.show()\n",
        "\n",
        "# plt.scatter(X[:,0], X[:,1],c=y, cmap=\"brg\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "class Layer_Dense():\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = 0.01*np.random.randn(n_inputs, n_neurons) # n denotes number of\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases # dot does cross product\n",
        "\n",
        "class Activation_ReLU():\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "class Activation_Softmax():\n",
        "  def forward(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    self.output = probabilities\n",
        "\n",
        "class Loss():\n",
        "  def calculate(self, output, y):\n",
        "    sample_losses = self.forward(output, y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss): # inherits loss class\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # avoid infinity issue\n",
        "\n",
        "    if len(y_true.shape) == 1: # passed scalar values\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "X,y = spiral_data(samples=100, classes=3)\n",
        "dense1 = Layer_Dense(2,3)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output[:5])\n",
        "\n",
        "loss_function = Loss_CategoricalCrossEntropy()\n",
        "loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# layer1 = Layer_Dense(2,5) # number of inputs/features = 4\n",
        "# activation1 = Activation_ReLU()\n",
        "\n",
        "# layer1.forward(X)\n",
        "# #print(layer1.output)\n",
        "\n",
        "# activation1.forward(layer1.output)\n",
        "# print(activation1.output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# step function, ex. y + { 1 if x>0, 0 if <= 0\n",
        "# output either a 0 or 1, regardless of inputs, weights, biases\n",
        "# alternative, Sigmoid activation function like y = 1/(1+e^-x)\n",
        "# however, has issue of vanishing gradient problem\n",
        "# rectified linear unit (reLU) activation function, y = { x if x> 0, 0 if x <= 0\n",
        "# can only fit linear functions, or for non-linear data, approximate with a lienar function\n",
        "# reLU close to being linear, but rectified (clipping at 0) makes it otherwise\n",
        "# optimizer: Daniel optimizer\n",
        "# increase weight, reLU slope gets steeper, decrease weight, gets shallower and if goes into the negatievs curves down\n",
        "# individual neurons become responsible for small sections of the ovreall neural network function\n",
        "\n",
        "X1 = [[1, 2, 3, 2.5],\n",
        "           [2.0, 5.0, -1.0,2.0],\n",
        "           [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "inputs4 = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "output4 = []\n",
        "\n",
        "for i in inputs4:\n",
        "  if i > 0:\n",
        "    output4.append(i)\n",
        "  elif i <= 0:\n",
        "    output4.append(max(0,i))\n",
        "print(output4)\n",
        "\n",
        "# softmax function\n",
        "\n",
        "# first step is to determine how wrong is a model (its loss as opposed to accuracy)\n",
        "layer_outputs_comparison1 = [4.8, 1.21, 2.385] # less loss\n",
        "layer_outputs_comparison2 = [4.8, 4.79, 4.25]\n",
        "\n",
        "# in example of image (cat/dog) data in classification model,\n",
        "# we want the 2 output values to be a probabaility distribution\n",
        "# (1) uniform from sample to sample\n",
        "# (2) from neuron to neuron things will be normalized\n",
        "# (3) can calculate rightness and wrongness\n",
        "# if everything was perfect, the classification would be a 1.0\n",
        "# applying the expoenentiation function (y=e^x) ensures that no value can be negative\n",
        "# but retaining the value/meaning of that negativity (its on a scale)\n",
        "\n",
        "layer_outputs2 = [[4.8, 1.21, 2.385], [8.9, -1.81, 0.2], [1.41, 1.051, 0.026]]\n",
        "\n",
        "#E = math.e # Euler's number\n",
        "\n",
        "exp_values = np.exp(layer_outputs2)\n",
        "norm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "print(norm_values)\n",
        "print(np.sum(layer_outputs2, axis=1, keepdims=True)) # axis on matrix is sum of columbs\n",
        "\n",
        "# an issue of expoennetiation is the explosion of values/growth of exponentiation function\n",
        "# (does not take too long to reach an overflow)\n",
        "\n",
        "\n",
        "# next step is to normalize the values\n",
        "# use function y = u / (summation n i=1 u_i)\n",
        "# (single outputs neuron's values divided by the sum of\n",
        "# all the other output neurons in that output layer)\n",
        "# gives probability distribution\n",
        "\n",
        "#  norm_base = sum(exp_values)\n",
        "#  norm_values = []\n",
        "\n",
        "# for value in exp_values:\n",
        "#   norm_values.append(value / norm_base)\n",
        "\n",
        "# print(norm_values)\n",
        "# print(sum(norm_values))\n",
        "\n",
        "# we now have the following implemented\n",
        "# input -> exponentiate -> normalize -> output\n",
        "# combination of exponentiation and normalization makes up the Softmax activation function\n",
        "# this is defined by S_i,j = e^(z_i,j) / (summation L l=1 e^(z_i,j))\n",
        "# we will have a batch of inputs and a batch of outputs\n",
        "\n",
        "\n",
        "# a neural network outputs a porbability distribution, predicts with a certain % confidence\n",
        "# optimizers make tweaks to weights and biases\n",
        "# an example of a loss function is mean absolute error (usually regression)\n",
        "# loss function is thus more useful/informative than accuracy\n",
        "# we know from training data what was the intended target value\n",
        "# the loss function of choice is Categorical Cross-Entropy\n",
        "# L i = - summation y y_i,j log(y(hat)_i,j)\n",
        "# L_i = sample loss value, i = i-th sample in a set, j = label/output index, y(hat) = predicted values\n",
        "# simplifies to L_i = -log(y(hat)_i, k) where k = target label index, index of correct class proabaility\n",
        "# one-hot encoding, vector is n classes long. vector is filled with zeros except at the index of the target class\n",
        "# ex. classes: 2, label: 1, one-hot: [0,1]\n",
        "# natural log, y = log_e x = ln(x)\n",
        "# for categorical cross entropy, we start by taking the negative sum of the target value times the log of\n",
        "# the predicted value for each of the values in the distributions\n",
        "\n",
        "\n",
        "# softmax_output = [0.7, 0.1, 0.2]\n",
        "# target_output = [1,0,0] #target_class = 0\n",
        "# loss = -(math.log(softmax_output[0])* target_output[0]+\n",
        "#          math.log(softmax_output[1])* target_output[1]+\n",
        "#          math.log(softmax_output[2])* target_output[2])\n",
        "# print(loss)\n",
        "\n",
        "# loss = math.log(softmax_output[0])\n",
        "# print(loss)\n",
        "# print(math.log(0.7))\n",
        "# print(math.log(0.5))\n",
        "\n",
        "\n",
        "\n",
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        "                            [0.1, 0.5, 0.4],\n",
        "                            [0.02, 0.9, 0.08]])\n",
        "class_targets = [0,1,1]\n",
        "predictions = np.argmax(softmax_outputs, axis=1)\n",
        "accuracy = np.mean(predictions == class_targets)\n",
        "print('acc:', accuracy)\n",
        "\n",
        "print(softmax_outputs[[0,1,2], class_targets])\n",
        "\n",
        "print(\"start here\")\n",
        "\n",
        "# optimization, adjustment of weights and biases\n",
        "# involves calculus componenets (derrivatives/tangent lines)\n",
        "# adjust parameters by calculuating the impact of those\n",
        "# parameters on the output of the function\n",
        "# for each sample we do a forward pass, calculating the loss at the end\n",
        "# then, for each weight and bias, individually, add the sma;l delta (change)\n",
        "# do another forward pass, calculating the loss again, revert the parameter\n",
        "# to its original state, choose a new one, add the delta, repeat the forward\n",
        "# pass and continue for every single weight and bias in the network\n",
        "# for multivariate functions we can use partial\n",
        "# derivatives (instead of doing multiple forward passes for the network)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49GgxFExCDRe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}